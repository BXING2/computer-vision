## General
This example demonstrates image generations using diffusion models. 

## Dataset
The dataset is CIFAR-10 from Torchvision. It consists of a train subset with 50K images and a test subset with 10K images. The image dimension is 32x32x3. There are 10 classes in the dataset. 80% and 20% train subset are used for training and validation, respectively. The entire test subset is used for evaluating the final model performance. 

Dataset Link: https://pytorch.org/vision/0.19/generated/torchvision.datasets.CIFAR10.html

## Model
The model is PPO (Proximal Policy Optimization) which belongs to policy gradient algorithm. It consists of an actor model and critic model. The actor model approximates a policy function for generating the action distributions. The critic model approximates a value function for evaluating the goodness of the selected action given the state. The model is optimized on the clipped surrogate objective function to avoid rapid updates on the parameters. For each iteration, a number of trajectories (a sequence of states and actions) are generated based on the current policy. Using quantities including action probalities, advantages, predicted values and returns, the policy and value model parameters are optimized. The updated policy is used again to generate new trajectories for the optimizetion at the next iteration.

## Evaluation
| Timestep=0 | Timestep=100 | Timestep=200 | Timestep=300 | Timestep=400 | Timestep=500 |
|---|---|---|---|---|---|
| <img src="figures/noise_image_0.tif" /> | <img src="figures/noise_image_100.tif" /> | <img src="figures/noise_image_200.tif" /> | <img src="figures/noise_image_300.tif" /> | <img src="figures/noise_image_400.tif" /> | <img src="figures/noise_image_500.tif" /> |

**Figure 1. Forward process of adding noises incrementally to the original image.**

| Timestep=500 | Timestep=400 | Timestep=300 | Timestep=200 | Timestep=100 | Timestep=0 |
|---|---|---|---|---|---|
| <img src="figures/denoise_image_500.tif" /> | <img src="figures/denoise_image_400.tif" /> | <img src="figures/denoise_image_300.tif" /> | <img src="figures/denoise_image_200.tif" /> | <img src="figures/denoise_image_100.tif" /> | <img src="figures/denoise_image_0.tif" /> |

**Figure 2. Backward process of removing noises incrementally from the pure noisy image followig normal distribution.**

| Real Images | Fake Images |
|---|---|
| <img src="figures/real_images.tif" width="500" /> | <img src="figures/fake_images.tif" width="500" /> |

**Figure 3. Demonstration of real images from the original dataset and fake images generated by the diffusion model.**

| 40 Iterations | 90 Iterations |
|---|---|
|<video src="https://github.com/user-attachments/assets/5774f724-26c2-416f-82bc-0e728f770125" height="200"></video> | <video src="https://github.com/user-attachments/assets/819be0f8-5c3c-4104-b463-e6a56da8c6de" height="200"></video> |

**Video 1. Movements of inverted double pendulum from models after training for 40 iterations (left) and 90 iterations (right).**

Figure 1,2 shows the average return and number of steps during training and testing stages. During training, the actor model is saved every 10 iterations. During testing, each saved model is used to generated 100 trajectories, each of which has at most 1000 steps. The average returns and number of steps are shown in Figure 2 for various checkpoints, indicating the enhanced model performance with the increasing of training iterations. 

The Video 1 shows two videos recording the movements of the inverted double pendulum from the model trained for 40 and 90 iterations, respectively. After training for 40 iterations (left video), the double pendulum can maintain stable within few steps but eventaully fails. However, after training for 90 iterations (right video), the double pendulum keeps stable during the 1000 steps applied.

## Reference
1. https://gymnasium.farama.org/index.html
2. Schulman, John, et al. "Proximal policy optimization algorithms." arXiv preprint arXiv:1707.06347 (2017).
3. https://pytorch.org/tutorials/intermediate/reinforcement_ppo.html
